{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "# reading file\n",
    "\n",
    "text = open('shakespeare.txt').read()\n",
    "print('Total characters: %d' %(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])\n",
    "# text = text[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocab: 65\n"
     ]
    }
   ],
   "source": [
    "# extracting unique elements in the text\n",
    "\n",
    "vocab = sorted(list(set(text)))\n",
    "print('Total vocab: %d' %(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "# a numpy series containing all the unique elements\n",
    "\n",
    "int_to_char = np.array(vocab)\n",
    "print(int_to_char)\n",
    "# print()\n",
    "# print(int_to_char.shape) # (65,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '$': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '3': 9,\n",
       " ':': 10,\n",
       " ';': 11,\n",
       " '?': 12,\n",
       " 'A': 13,\n",
       " 'B': 14,\n",
       " 'C': 15,\n",
       " 'D': 16,\n",
       " 'E': 17,\n",
       " 'F': 18,\n",
       " 'G': 19,\n",
       " 'H': 20,\n",
       " 'I': 21,\n",
       " 'J': 22,\n",
       " 'K': 23,\n",
       " 'L': 24,\n",
       " 'M': 25,\n",
       " 'N': 26,\n",
       " 'O': 27,\n",
       " 'P': 28,\n",
       " 'Q': 29,\n",
       " 'R': 30,\n",
       " 'S': 31,\n",
       " 'T': 32,\n",
       " 'U': 33,\n",
       " 'V': 34,\n",
       " 'W': 35,\n",
       " 'X': 36,\n",
       " 'Y': 37,\n",
       " 'Z': 38,\n",
       " 'a': 39,\n",
       " 'b': 40,\n",
       " 'c': 41,\n",
       " 'd': 42,\n",
       " 'e': 43,\n",
       " 'f': 44,\n",
       " 'g': 45,\n",
       " 'h': 46,\n",
       " 'i': 47,\n",
       " 'j': 48,\n",
       " 'k': 49,\n",
       " 'l': 50,\n",
       " 'm': 51,\n",
       " 'n': 52,\n",
       " 'o': 53,\n",
       " 'p': 54,\n",
       " 'q': 55,\n",
       " 'r': 56,\n",
       " 's': 57,\n",
       " 't': 58,\n",
       " 'u': 59,\n",
       " 'v': 60,\n",
       " 'w': 61,\n",
       " 'x': 62,\n",
       " 'y': 63,\n",
       " 'z': 64}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a dict pointing char to index\n",
    "\n",
    "char_to_int = dict((ch,i) for i,ch in enumerate(vocab))\n",
    "char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting input format from char to int\n",
    "\n",
    "text_as_int = np.array([char_to_int[ch] for ch in text])\n",
    "# text_as_int.shape # (1115394,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
       "       44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n",
       "       52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1,\n",
       "       51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31,\n",
       "       54, 43, 39, 49,  6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56,\n",
       "       57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int [:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# Tensors can be explicitly converted to NumPy ndarrays by invoking the .numpy() method on them\n",
    "\n",
    "seq_length = 100 # number of time-steps; length of each example\n",
    "examples_per_epoch = len(text)//seq_length  # 11153; number of sequences/examples per epoch\n",
    "\n",
    "# tf.data.Dataset.from_tensor_slices function converts numpy dataset into tensorflow dataset\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for char in char_dataset.take(5):\n",
    "    print(int_to_char[char.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(char_dataset) # tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n"
     ]
    }
   ],
   "source": [
    "# dividing text into sequences/examples each of size = 101\n",
    "# input : 0-100 ; output : 1-101\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for char in sequences.take(3):\n",
    "    print(repr(''.join(int_to_char[char.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each sequence into input[0-100] and target[1-101]\n",
    "\n",
    "def split_into_input_target(chunk):\n",
    "    inputs = chunk[:-1]\n",
    "    targets = chunk[1:]\n",
    "    return inputs, targets\n",
    "\n",
    "dataset = sequences.map(split_into_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in dataset.take(1):\n",
    "    print('Input data:', repr(''.join(int_to_char[inputs.numpy()])))\n",
    "    print('Target data:', repr(''.join(int_to_char[targets.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 18 ('F')\n",
      "  expected output: 47 ('i')\n",
      "Step    1\n",
      "  input: 47 ('i')\n",
      "  expected output: 56 ('r')\n",
      "Step    2\n",
      "  input: 56 ('r')\n",
      "  expected output: 57 ('s')\n",
      "Step    3\n",
      "  input: 57 ('s')\n",
      "  expected output: 58 ('t')\n",
      "Step    4\n",
      "  input: 58 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(inputs[:5], targets[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(int_to_char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(int_to_char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples_per_epoch # 11153"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "\n",
    "batch_size = 64\n",
    "steps_per_epoch = examples_per_epoch//batch_size # no. of batches per epoch ; 174\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements)\n",
    "buffer_size = 10000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "    import functools\n",
    "    rnn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        rnn(rnn_units,\n",
    "            return_sequences=True,\n",
    "            recurrent_initializer='glorot_uniform',\n",
    "            stateful=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    examples_batch_predictions = model(input_example_batch)\n",
    "    print(examples_batch_predictions.shape, '# (batch_size, sequence_length, vocab_size)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3935232   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,018,497\n",
      "Trainable params: 4,018,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the first example in the batch\n",
    "\n",
    "sample_indices = tf.random.categorical(examples_batch_predictions[0], num_samples=1)\n",
    "\n",
    "# Given a tensor input, 'tf.squeeze' returns a tensor of the same type\n",
    "# with all dimensions of size 1 removed. If you don't want to remove all\n",
    "# size 1 dimensions, you can remove specific size 1 dimensions by specifying axis\n",
    "# e.g.- here last tensor is being removed\n",
    "\n",
    "# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]; tf.shape(tf.squeeze(t))  # [2, 3]\n",
    "# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]; tf.shape(tf.squeeze(t, [2, 4]))  # [1, 2, 3, 1]\n",
    "# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]; tf.shape(tf.squeeze(t, [-1]))  # [1, 2, 1, 3, 1]\n",
    "sample_indices = tf.squeeze(sample_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48, 38, 43, 32, 61,  5, 53,  0, 15, 39, 43, 49, 11, 60, 47, 33, 25,\n",
       "       63, 29, 45, 51, 10, 43,  0, 59, 64, 30, 37, 45, 58, 21, 50, 48,  2,\n",
       "       43, 34, 64, 44,  1, 61, 13, 40,  9, 53, 38, 49,  8, 45, 31, 58,  0,\n",
       "       14, 55,  0, 59, 23, 20, 40, 26, 58, 19, 40,  5, 32,  6, 34, 25, 61,\n",
       "       40, 64, 13, 55, 31, 33, 34,  5, 36, 33, 63, 63, 23, 12, 23, 16, 57,\n",
       "       36, 41, 29,  6, 45, 52, 55, 34, 48, 11, 32,  5, 34, 42, 57])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This gives us, at each timestep, a prediction of the next character index:\n",
    "\n",
    "sample_indices # (100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \"me: say that she were gone,\\nGiven to the fire, a moiety of my rest\\nMight come to me again. Who's the\"\n",
      "\n",
      "Next Char Predictions: \n",
      " \"jZeTw'o\\nCaek;viUMyQgm:e\\nuzRYgtIlj!eVzf wAb3oZk.gSt\\nBq\\nuKHbNtGb'T,VMwbzAqSUV'XUyyK?KDsXcQ,gnqVj;T'Vds\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\", repr(\"\".join(int_to_char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(int_to_char[sample_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100)  # (batch_size, sequence_length)\n",
      "scalar_loss:  4.174599\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "  \n",
    "example_batch_loss = loss(target_example_batch, examples_batch_predictions)\n",
    "print('Prediction shape: ', example_batch_loss.shape, ' # (batch_size, sequence_length)')\n",
    "print('scalar_loss: ',example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(), loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = 'training_checkpoints'\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174/174 [==============================] - 722s 4s/step - loss: 2.6747\n"
     ]
    }
   ],
   "source": [
    "# steps_per_epoch = 174\n",
    "epochs = 1\n",
    "history = model.fit(dataset.repeat(), epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_checkpoints/ckpt_1'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3935232   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 4,018,497\n",
      "Trainable params: 4,018,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Number of chars to generate\n",
    "    num_generate = 1000\n",
    "    \n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char_to_int[s] for s in start_string] # a numpy list\n",
    "#     print(input_eval) # [30, 27, 25, 17, 27, 10, 1]\n",
    "    \n",
    "    # Given a tensor input, this operation inserts a dimension \n",
    "    # of 1 at the dimension index axis of input's shape.\n",
    "    input_eval = tf.expand_dims(input_eval, 0) # a tensor\n",
    "#     print(input_eval.numpy()) # [[30 27 25 17 27 10  1]]\n",
    "#     print(input_eval.shape) # (1, 7)\n",
    "    \n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "    \n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    temperature = 1.0\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "#         print(predictions.numpy())\n",
    "#         print(predictions.shape) # (1, 7, 65)\n",
    "        \n",
    "        # Remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "#         print(predictions.numpy())\n",
    "#         print(predictions.shape) # (7, 65)\n",
    "        \n",
    "        # Using a multinomial distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "#         print(tf.multinomial(predictions, num_samples=1).numpy())\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "#         print(predicted_id) # 20\n",
    "#         print(predicted_id.shape) # () ----> scalar\n",
    "        \n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "#         print(input_eval.numpy()) # [[20]]\n",
    "#         print(input_eval.shape) # (1, 1)\n",
    "        \n",
    "#         print(int_to_char[predicted_id]) # H\n",
    "        text_generated.append(int_to_char[predicted_id])\n",
    "        \n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0704 11:37:33.295826 139977090963264 deprecation.py:323] From <ipython-input-34-105c652492eb>:36: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: fer,\n",
      "Ove aly pint to wiced ase ilssean\n",
      "I ky joun toreod as yourof'e, anstlave\n",
      "Sinf;\n",
      "For of ald by bellet?\n",
      "Hese to boontospeth'd be sit mes, and theak; a vederpu thtoll wingher mims, ad mingar youk.\n",
      "\n",
      "MENLUS:\n",
      "Srotheck, yourtrresceres, I thes of un'd lame fourtdent wheld his in lles.;\n",
      "Site me munis boody tit math rold theeeld a dele\n",
      "knoll vas ressen:\n",
      "No so hor fagd.\n",
      "\n",
      "SLARGES:\n",
      "Mekinot sest yot hand roursin\n",
      "Whangn sely ho yount plise, fore cirgur;\n",
      "Nim hour't boonith, saie il igely heaf\n",
      "ret, andarge ust cascenwed\n",
      "Mothy in ard jeemss.\n",
      "\n",
      "JLANK OF BHUCAON:\n",
      "Sim, bengr coukun lacse'n,\n",
      "What bote cone; thut and mist forpren the ardinl thoun a fores's, fored sithingee.;\n",
      "Sathir leige't rome I mavcuntt should woth;\n",
      "I live qo mit his; stist tricpanh.\n",
      "\n",
      "SUTY ROU:\n",
      "Hist lite a youd stien late tot nott,\n",
      "Ufrem's frads,\n",
      "Bod be sear love thes ibguane aros the pasten learens\n",
      "Merlien\n",
      "And brictue tut in himkoned youk mathe I his wivl therfo, of ley tighe moblest.\n",
      "\n",
      "GFUFhrunom rome\n",
      "Shit will ane axl thit hy anf thil\n"
     ]
    }
   ],
   "source": [
    "# print(generate_text(model, start_string=u\"ROMEO: \"))\n",
    "print(generate_text(model, start_string=\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
