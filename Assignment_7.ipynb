{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GntWqFyJmkjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgymjBgrmkjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUYqo6vjmkjk",
        "colab_type": "code",
        "outputId": "34721955-4df3-4cd3-a096-621f7009a743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# reading file\n",
        "\n",
        "text = open('shakespeare.txt').read()\n",
        "print('Total characters: %d' %(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total characters: 1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdgW4OeVmkjo",
        "colab_type": "code",
        "outputId": "8f0500f0-5061-4de3-9f17-3ea50b5523cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print(text[:250])\n",
        "# text = text[:10000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tq_JUyEmkjr",
        "colab_type": "code",
        "outputId": "2c0535f4-7381-453f-f407-129c0f033787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# extracting unique elements in the text\n",
        "\n",
        "vocab = sorted(list(set(text)))\n",
        "print('Total vocab: %d' %(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total vocab: 65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-sC1-Sgmkju",
        "colab_type": "code",
        "outputId": "483ada6c-e0fd-447a-fe95-e1128d6c00d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# a numpy series containing all the unique elements\n",
        "\n",
        "int_to_char = np.array(vocab)\n",
        "print(int_to_char)\n",
        "# print()\n",
        "# print(int_to_char.shape) # (65,)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
            " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
            " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
            " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umldMJiTmkjx",
        "colab_type": "code",
        "outputId": "46d99718-01a2-4cda-a3a7-a07abd3c7ec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# a dict pointing char to index\n",
        "\n",
        "char_to_int = dict((ch,i) for i,ch in enumerate(vocab))\n",
        "char_to_int"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '$': 3,\n",
              " '&': 4,\n",
              " \"'\": 5,\n",
              " ',': 6,\n",
              " '-': 7,\n",
              " '.': 8,\n",
              " '3': 9,\n",
              " ':': 10,\n",
              " ';': 11,\n",
              " '?': 12,\n",
              " 'A': 13,\n",
              " 'B': 14,\n",
              " 'C': 15,\n",
              " 'D': 16,\n",
              " 'E': 17,\n",
              " 'F': 18,\n",
              " 'G': 19,\n",
              " 'H': 20,\n",
              " 'I': 21,\n",
              " 'J': 22,\n",
              " 'K': 23,\n",
              " 'L': 24,\n",
              " 'M': 25,\n",
              " 'N': 26,\n",
              " 'O': 27,\n",
              " 'P': 28,\n",
              " 'Q': 29,\n",
              " 'R': 30,\n",
              " 'S': 31,\n",
              " 'T': 32,\n",
              " 'U': 33,\n",
              " 'V': 34,\n",
              " 'W': 35,\n",
              " 'X': 36,\n",
              " 'Y': 37,\n",
              " 'Z': 38,\n",
              " 'a': 39,\n",
              " 'b': 40,\n",
              " 'c': 41,\n",
              " 'd': 42,\n",
              " 'e': 43,\n",
              " 'f': 44,\n",
              " 'g': 45,\n",
              " 'h': 46,\n",
              " 'i': 47,\n",
              " 'j': 48,\n",
              " 'k': 49,\n",
              " 'l': 50,\n",
              " 'm': 51,\n",
              " 'n': 52,\n",
              " 'o': 53,\n",
              " 'p': 54,\n",
              " 'q': 55,\n",
              " 'r': 56,\n",
              " 's': 57,\n",
              " 't': 58,\n",
              " 'u': 59,\n",
              " 'v': 60,\n",
              " 'w': 61,\n",
              " 'x': 62,\n",
              " 'y': 63,\n",
              " 'z': 64}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67Jf3Yqzmkj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting input format from char to int\n",
        "\n",
        "text_as_int = np.array([char_to_int[ch] for ch in text])\n",
        "# text_as_int.shape # (1115394,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-nOviLamkj2",
        "colab_type": "code",
        "outputId": "1369ccd5-f865-4e3c-e463-f92c8305ef0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "text_as_int [:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
              "       44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n",
              "       52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1,\n",
              "       51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31,\n",
              "       54, 43, 39, 49,  6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56,\n",
              "       57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzQ133Mbmkj5",
        "colab_type": "code",
        "outputId": "ee8115a1-6d84-4b37-88bf-7c6d82b952da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Tensors can be explicitly converted to NumPy ndarrays by invoking the .numpy() method on them\n",
        "\n",
        "seq_length = 100 # number of time-steps; length of each example\n",
        "examples_per_epoch = len(text)//seq_length  # 11153; number of sequences/examples per epoch\n",
        "\n",
        "# tf.data.Dataset.from_tensor_slices function converts numpy dataset into tensorflow dataset\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for char in char_dataset.take(5):\n",
        "    print(int_to_char[char.numpy()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p82MpUWPmkj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# type(char_dataset) # tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3P7XzOgmkj-",
        "colab_type": "code",
        "outputId": "c0af5112-b75b-42c3-f34d-91ed63a76f9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# dividing text into sequences/examples each of size = 101\n",
        "# input : 0-100 ; output : 1-101\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for char in sequences.take(3):\n",
        "    print(repr(''.join(int_to_char[char.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWo1Q9HxmkkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split each sequence into input[0-100] and target[1-101]\n",
        "\n",
        "def split_into_input_target(chunk):\n",
        "    inputs = chunk[:-1]\n",
        "    targets = chunk[1:]\n",
        "    return inputs, targets\n",
        "\n",
        "dataset = sequences.map(split_into_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM0LkgFzmkkC",
        "colab_type": "code",
        "outputId": "b0a2e2a8-6611-4dd3-d55b-906928f661c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "for inputs, targets in dataset.take(1):\n",
        "    print('Input data:', repr(''.join(int_to_char[inputs.numpy()])))\n",
        "    print('Target data:', repr(''.join(int_to_char[targets.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data: 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ql_B6hZ8mkkF",
        "colab_type": "code",
        "outputId": "ad75cf6a-96f1-4dab-ca6e-07d358036995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(inputs[:5], targets[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(int_to_char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(int_to_char[target_idx])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdpmS_Z6mkkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# examples_per_epoch # 11153"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlGPmQWumkkL",
        "colab_type": "text"
      },
      "source": [
        "# Create training batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtUEZZHwmkkL",
        "colab_type": "code",
        "outputId": "72f31a30-ee78-4a2a-d7e6-9672779c084a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Batch size\n",
        "\n",
        "batch_size = 64\n",
        "steps_per_epoch = examples_per_epoch//batch_size # no. of batches per epoch ; 174\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences, \n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
        "# it maintains a buffer in which it shuffles elements)\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXHHfenZmkkO",
        "colab_type": "text"
      },
      "source": [
        "# Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uImwRfYdmkkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T4rWp0tmkkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if tf.test.is_gpu_available():\n",
        "    rnn = tf.keras.layers.CuDNNGRU\n",
        "else:\n",
        "    import functools\n",
        "    rnn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m0RTe9nmkkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "        rnn(rnn_units,\n",
        "            return_sequences=True,\n",
        "            recurrent_initializer='glorot_uniform',\n",
        "            stateful=True),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ0RHJi4mkkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XtqOxWbmkkZ",
        "colab_type": "text"
      },
      "source": [
        "# Try the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46vH-KjWmkkZ",
        "colab_type": "code",
        "outputId": "616edbc5-1d62-419b-b700-eb256b100aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    examples_batch_predictions = model(input_example_batch)\n",
        "    print(examples_batch_predictions.shape, '# (batch_size, sequence_length, vocab_size)')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaBrZqaUmkkc",
        "colab_type": "code",
        "outputId": "d892385b-50ec-4d72-9387-6d263ece7693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3935232   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,018,497\n",
            "Trainable params: 4,018,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyNAREknmkke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For the first example in the batch\n",
        "\n",
        "sample_indices = tf.random.categorical(examples_batch_predictions[0], num_samples=1)\n",
        "\n",
        "# Given a tensor input, 'tf.squeeze' returns a tensor of the same type\n",
        "# with all dimensions of size 1 removed. If you don't want to remove all\n",
        "# size 1 dimensions, you can remove specific size 1 dimensions by specifying axis\n",
        "# e.g.- here last tensor is being removed\n",
        "\n",
        "# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]; tf.shape(tf.squeeze(t))  # [2, 3]\n",
        "# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]; tf.shape(tf.squeeze(t, [2, 4]))  # [1, 2, 3, 1]\n",
        "# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]; tf.shape(tf.squeeze(t, [-1]))  # [1, 2, 1, 3, 1]\n",
        "sample_indices = tf.squeeze(sample_indices, axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUKjZdCFmkkg",
        "colab_type": "code",
        "outputId": "bfb8e986-3f1b-4c38-d978-31821ec58ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# This gives us, at each timestep, a prediction of the next character index:\n",
        "\n",
        "sample_indices # (100, )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10, 52, 24, 61, 43, 24,  9, 31, 60,  7, 61,  2, 20,  1, 34,  4, 34,\n",
              "       46, 54, 42, 38,  6, 40, 59, 12,  1, 43,  7, 47,  6,  6, 11, 39, 48,\n",
              "       51,  7, 11, 45, 44,  0, 23, 41, 43, 58, 62, 14, 26, 40, 54, 42, 44,\n",
              "       36, 55, 57, 47, 64, 38, 20, 28,  7, 35, 41, 54,  8, 52,  7,  1, 19,\n",
              "       37, 55, 34, 51, 41, 54, 20, 22,  4, 38, 40, 29, 21, 61, 55, 22, 58,\n",
              "       44, 41,  7,  9, 37,  5, 15, 24,  4, 48, 35, 29, 28, 22, 45])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx9Z1rnrmkki",
        "colab_type": "code",
        "outputId": "ca7d7c86-b7f8-49ce-b113-b90849db3239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(\"Input:\", repr(\"\".join(int_to_char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(int_to_char[sample_indices])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \"uried in the king's highway,\\nSome way of common trade, where subjects' feet\\nMay hourly trample on th\"\n",
            "\n",
            "Next Char Predictions: \n",
            " \":nLweL3Sv-w!H V&VhpdZ,bu? e-i,,;ajm-;gf\\nKcetxBNbpdfXqsizZHP-Wcp.n- GYqVmcpHJ&ZbQIwqJtfc-3Y'CL&jWQPJg\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osWlRsstmkkl",
        "colab_type": "text"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3sMptlSmkkl",
        "colab_type": "code",
        "outputId": "275f9536-6dcc-4259-97c3-e1b1f15b3576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "  \n",
        "example_batch_loss = loss(target_example_batch, examples_batch_predictions)\n",
        "print('Prediction shape: ', example_batch_loss.shape, ' # (batch_size, sequence_length)')\n",
        "print('scalar_loss: ',example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100)  # (batch_size, sequence_length)\n",
            "scalar_loss:  4.1745753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBig6x_3mkko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=tf.train.AdamOptimizer(), loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oFB36Jrmkkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = 'training_checkpoints'\n",
        "\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix,\n",
        "    save_weights_only = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zW6it_Hmkks",
        "colab_type": "code",
        "outputId": "38030160-7f3c-4987-d3d8-315073da2364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# steps_per_epoch = 174\n",
        "epochs = 5\n",
        "history = model.fit(dataset.repeat(), epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "174/174 [==============================] - 1729s 10s/step - loss: 2.6763\n",
            "Epoch 2/5\n",
            "174/174 [==============================] - 1717s 10s/step - loss: 1.9334\n",
            "Epoch 3/5\n",
            "174/174 [==============================] - 1716s 10s/step - loss: 1.6793\n",
            "Epoch 4/5\n",
            "174/174 [==============================] - 1721s 10s/step - loss: 1.5377\n",
            "Epoch 5/5\n",
            "147/174 [========================>.....] - ETA: 4:28 - loss: 1.4551Epoch 1/5\n",
            "174/174 [==============================] - 1729s 10s/step - loss: 2.6763\n",
            "Epoch 2/5\n",
            "174/174 [==============================] - 1717s 10s/step - loss: 1.9334\n",
            "Epoch 3/5\n",
            "174/174 [==============================] - 1716s 10s/step - loss: 1.6793\n",
            "Epoch 4/5\n",
            "174/174 [==============================] - 1721s 10s/step - loss: 1.5377\n",
            "Epoch 5/5\n",
            "174/174 [==============================] - 1735s 10s/step - loss: 1.4495\n",
            "174/174 [==============================] - 1735s 10s/step - loss: 1.4495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt1JOaVymkku",
        "colab_type": "text"
      },
      "source": [
        "# Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r-ISXYHmkkv",
        "colab_type": "code",
        "outputId": "021045b4-2520-4edd-9d67-4abc0b91033a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'training_checkpoints/ckpt_5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH4Pck5Hmkkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-dUcKSHmkk0",
        "colab_type": "code",
        "outputId": "9f84aecf-e755-4f53-d393-d6055c9425ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3935232   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,018,497\n",
            "Trainable params: 4,018,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMTg3kKDmkk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Number of chars to generate\n",
        "    num_generate = 1000\n",
        "    \n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char_to_int[s] for s in start_string] # a numpy list\n",
        "#     print(input_eval) # [30, 27, 25, 17, 27, 10, 1]\n",
        "    \n",
        "    # Given a tensor input, this operation inserts a dimension \n",
        "    # of 1 at the dimension index axis of input's shape.\n",
        "    input_eval = tf.expand_dims(input_eval, 0) # a tensor\n",
        "#     print(input_eval.numpy()) # [[30 27 25 17 27 10  1]]\n",
        "#     print(input_eval.shape) # (1, 7)\n",
        "    \n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "    \n",
        "    # Low temperatures results in more predictable text.\n",
        "    # Higher temperatures results in more surprising text.\n",
        "    temperature = 1.0\n",
        "    \n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "#         print(predictions.numpy())\n",
        "#         print(predictions.shape) # (1, 7, 65)\n",
        "        \n",
        "        # Remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "#         print(predictions.numpy())\n",
        "#         print(predictions.shape) # (7, 65)\n",
        "        \n",
        "        # Using a multinomial distribution to predict the word returned by the model\n",
        "        predictions = predictions / temperature\n",
        "#         print(tf.multinomial(predictions, num_samples=1).numpy())\n",
        "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "#         print(predicted_id) # 20\n",
        "#         print(predicted_id.shape) # () ----> scalar\n",
        "        \n",
        "        # We pass the predicted word as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "#         print(input_eval.numpy()) # [[20]]\n",
        "#         print(input_eval.shape) # (1, 1)\n",
        "        \n",
        "#         print(int_to_char[predicted_id]) # H\n",
        "        text_generated.append(int_to_char[predicted_id])\n",
        "        \n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "XGQqpnTpmkk5",
        "colab_type": "code",
        "outputId": "d6106586-ab3f-48af-fa25-0dcf42cc59df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "# print(generate_text(model, start_string=u\"ROMEO: \"))\n",
        "print(generate_text(model, start_string=\"ROMEO: \"))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0704 08:35:49.018097 140170663364480 deprecation.py:323] From <ipython-input-34-105c652492eb>:36: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ROMEO: I was\n",
            "Maken him kingrish'd as dnown: but this is the viely sour!\n",
            "\n",
            "Veiden: I'll tell thee her frows, to your hermits, and the till should ring by\n",
            "it in that ay, thereon night, my lord.\n",
            "The varat! But too lay him,\n",
            "You know my strusk is one fair point: I do gan us yet?\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "None watching others. Do much many masters, and thou dast\n",
            "You: good my bose before his swift,\n",
            "From whom to yea, or now to fribe,\n",
            "Or all man, do not sighs for a merry,\n",
            "Who's tho gaves door majesty, till thee:\n",
            "I soonet dot?\n",
            "\n",
            "GEORTES:\n",
            "I saws and cause, depair;\n",
            "For newnresed to her hagry in the crown.\n",
            "\n",
            "KING EDWARD II:\n",
            "Adswer to fash, to of thees with him.\n",
            "I pray you, my anghy day.\n",
            "You knowly confess, quite to find\n",
            "o'e my love's mather flostly and but thee.\n",
            "\n",
            "ALO:\n",
            "I take your late most officer:\n",
            "That speak again. I say, my chapter, to the chird?\n",
            "\n",
            "Numbert:\n",
            "O, I shall but concees to seals make the postring plack my love.\n",
            "I they are noble and upon the walls? ip to thine in purcus.\n",
            "\n",
            "MASCIUS:\n",
            "No, but I'll see up,\n",
            "An\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlnX-Ggpmkk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}